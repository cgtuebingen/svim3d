{
    "chapters": [
        {
            "name": "Abstract",
            "styling": {
                "hideHeading": true
            },
            "introduction": [
                {
                    "type": "text",
                    "content": [
                        {
                            "type": "plain_text",
                            "content": "We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media."
                        }
                    ]
                }
            ],
            "sections": [],
            "paragraphs": []
        },
        {
            "name": "Overview",
            "introduction": [
                {
                    "type": "text",
                    "content": [
                        {
                            "type": "plain_text",
                            "content": "Stable Video Materials 3D (SViM3D) is a probabilistic generative diffusion model that tackles object-centric inverse rendering from a single image. Conditioned on a camera pose sequence it generates both high-quality appearances as well as the corresponding multi-view consistent material properties."
                        },
                        {
                            "type": "plain_text",
                            "content": "Unlike prior approaches that decouple material estimation from 3D reconstruction, SViM3D is the first camera-controllable multi-view model that can produce fully spatially-varying PBR parameters, RGB color and surface normals simultaneously. The additional output can be leveraged in various applications, hence we consider SViM3D a foundational model that provides a unified neural prior for both 3D reconstruction and material understanding. SViM3Dâ€™s output can be used to relight the views directly, perform material edits or to generate full 3D assets by lifting the multi-view material parameters to 3D. As 3D training data paired with material parameters is scarce, we leverage the accumulated world knowledge of a latent video diffusion model. Specifically, we adapt"
                        }
                        {
                            "type": "link_text",
                            "content": "SV3D [1]",
                            "link": "https://sv3d.github.io/"
                        },
                        {
                            "type": "plain_text",
                            "content": " a video diffusion model fine-tuned for camera-control by incorporating several crucial modifications: "
                        },
                    ]
                },
                {
                    "type": "list",
                    "content": [
                        {
                            "type": "text",
                            "content": [
                                {
                                    "type": "plain_text",
                                    "content": "Multi-illumination multi-view training dataset: We render a high-quality photorealistic synthetic dataset, capturing the complexity of real-world lighting and material variations."
                                }
                            ]
                        },
                        {
                            "type": "text",
                            "content": [
                                {
                                    "type": "plain_text",
                                    "content": "Material latent representation: We treat the material parameters and surface normals as images reusing the image-based autoencoder to encode all inputs into unified latents."
                                }
                            ]
                        },
                        {
                            "type": "text",
                            "content": [
                                {
                                    "type": "plain_text",
                                    "content": "Adapted UNet architecture: We make crucial changes in the core architecture and training scheme to smoothly adapt from image to image+material+normal generation."
                                }
                            ]
                        },
                    ]
                },
                {
                    "type": "figure",
                    "id": "overview"
                }
            ],
            "sections": [],
            "paragraphs": []
        },
        {
            "name": "Method",
            "introduction": [
                {
                    "type": "text",
                    "content": [
                        {
                            "type": "plain_text",
                            "content": "The figure below outlines our pipeline which can be divided into the material video generation and the reconstruction part."
                        }
                    ]
                },
                {
                    "type": "figure",
                    "id": "pipeline"
                }
            ],
            "sections": [],
            "paragraphs": [
                {
                    "type": "paragraph",
                    "name": "Material video generation",
                    "contents": [
                        {
                            "type": "text",
                            "content": [
                                {
                                    "type": "plain_text",
                                    "content": "We reuse the pre-trained VAE to define the latent encoding for the additional material channels. The denoising Unet is fine-tuned to process the new set of in- and outputs keeping the conditioning from the video model (SV3D) intact. For efficient training we employ a staged adaption using our new material dataset and several heuristics to filter the available object data w.r.t. the material quality. "
                                },
                            ]
                        },
                        {
                            "type": "figure",
                            "id": "svim3d_material_video"
                        }
                    ]
                },
                {
                    "type": "paragraph",
                    "name": "Reconstruction",
                    "contents": [
                        {
                            "type": "text",
                            "content": [
                                {
                                    "type": "plain_text",
                                    "content": "The output videos can then be used as pseudo ground truth in a 3D optinization framework which we build on our efficient illumination representation together with our novel mechanism to improve robustness on inconcistent generations. The reconstruction includes the following steps:"
                                },
                            ]
                        },
                        {
                            "type": "list",
                            "content": [
                                {
                                    "type": "text",
                                    "content": [
                                        {
                                            "type": "plain_text",
                                            "content": "An illumination representation is pre-optimized using the orbital video M to initialize Phase 1."
                                        }
                                    ]
                                },
                                {
                                    "type": "text",
                                    "content": [
                                        {
                                            "type": "plain_text",
                                            "content": "A modified Instant-NGP is optimized using a photometric rendering loss relying on the jointly optimized illumination and supervision from the reference views."
                                        }
                                    ]
                                },
                                {
                                    "type": "text",
                                    "content": [
                                        {
                                            "type": "plain_text",
                                            "content": "We optimize a DMTet representation initialized from the results of Phase 1 via marching cubes."
                                        }
                                    ]
                                },
                                {
                                    "type": "text",
                                    "content": [
                                        {
                                            "type": "plain_text",
                                            "content": "A mesh is finally extracted, UV unwrapped using xatlas and all textures baked."
                                        }
                                    ]
                                },
                            ]
                        },
                    ]
                },
            ]
        },
        {
            "name": "Results",
            "introduction": [
                {
                    "type": "text",
                    "content": [
                        {
                            "type": "plain_text",
                            "content": "The parametric material model allows for direct relighting and editing in image space. Try for yourself using the demp below."
                        }
                    ]
                }
            ],
            "sections": [],
            "paragraphs": [
                {
                    "type": "paragraph",
                    "name": "Applications",
                    "contents": [
                        {
                            "type": "text",
                            "content": [
                                {
                                    "type": "plain_text",
                                    "content": "We show an example from our "
                                },
                                {
                                    "type": "link_text",
                                    "content": "Poly Haven [2]",
                                    "link": "https://polyhaven.com//"
                                },
                                {
                                    "type": "plain_text",
                                    "content": " test dataset. Using our multi-view consistent material video we can relight novel views based on the camera conditioning and our efficient illumination representation."
                                }
                            ]
                        },
                        {
                            "type": "figure",
                            "id": "svim3d_2d_relighting"
                        }
                    ]
                },
                {
                    "type": "paragraph",
                    "name": "Comparison to Prior Works",
                    "contents": [
                        {
                            "type": "text",
                            "content": [
                                {
                                    "type": "plain_text",
                                    "content": "Note the improved consistency and level of detail of our conditioned multi-view material generation compared to other prior works."
                                }
                            ]
                        },
                        {
                            "type": "figure",
                            "id": "results_multi_view"
                        }
                    ]
                },
                {
                    "type": "paragraph",
                    "name": "Demo",
                    "contents": [
                        {
                            "type": "text",
                            "content": [
                                {
                                    "type": "plain_text",
                                    "content": "Upload your image and environment map or use one of the examples to try the PBR material generation and 2D relighting pipeline."
                                }
                            ]
                        }
                    ]
                }
            ]
        }
    ]
}